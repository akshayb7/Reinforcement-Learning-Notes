{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay! So we now know how to frame any reinforcement problem in terms of a Markov Decision Process (MPD) as: \n",
    "- A set of states _S_\n",
    "- A set of actions _A_\n",
    "- A set of rewards _R_\n",
    "- A one-dynamic step of the environment, p(s',r | s,a) = P(S(t+1) = s', R(t+1) = r | S(t) = s, A(t) = a)\n",
    "- A discount rate γ∈[0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what will be the solution to it? Well, to find the solution all we need is a series of actions that needs to be learned by the agent towards the pursuit of its goal, in context with its current state and the states that follow.\n",
    "\n",
    "Therefore, as long as the agent can learn the appropriate action to the environment states that it can observe, we have a solution to our problem. This leads to the idea of a policy defining these actions. A policy can be:\n",
    "- Deterministic\n",
    "- Stochastic\n",
    "\n",
    "**Deterministic Policy:** It is basically a mapping of states to the corresponding actions which should be taken and can be represented as __π:S→A__. Example: π(low) = recharge , π(high) = explore\n",
    "\n",
    "**Stochastic Policy:** A stochastic policy basically tells us the probability of an action(for all the possible actions in the action space) being taken by an agent while in a particular state and is represented as __π:S×A→[0,1]__. Example: π(recharge ∣ low) = 0.5, π(wait ∣ low) = 0.4, π(search ∣ low) = 0.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Any deterministic Policy can be represented as a Stochastic Policy (Prob. for that particular action-state combination will be 1).\n",
    "\n",
    "Now that we know how to define the policy, another question arises. How to get the best policy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State-Value Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the best policy, we first need to define something called a **state-value function**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each state in the state space, the state-value function yields the expected return if the agent starts in that state and then follows the specified policy for all the timesteps, till termiation of that episode. It is denoted as Vπ.\n",
    "\n",
    "**Note:** The state-value function corresponds to a particular policy and changes if there is a change in the policy.\n",
    "\n",
    "It can be written down in notation form as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Images\\State-Value-Function.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman Expectation Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above definition of State value function, the Bellman equation states that to get the value of any state, we just need the expected sum of the immediate rewards and the discounted value of the state that follows. (We calculate the expected sum because generally, we are not 100% sure of the expected reward and the state that follows.)\n",
    "\n",
    "Vπ(s) = Eπ [G(t) | S(t) = s]\n",
    "      \n",
    "      = Eπ [R(t+1) + γR(t+2) + .... | S(t) = s]\n",
    "      \n",
    "      = Eπ [R(t+1) + γVπ(S(t+1)) | S(t) = s]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'Images\\Bellman-Equation.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But how do we calculate these expected values? This is where our policy comes into play.\n",
    "\n",
    "For a *Deterministic policy*:\n",
    "\n",
    "Vπ(s) = Eπ [R(t+1) + γVπ(S(t+1)) | S(t) = s]\n",
    "\n",
    "can be re-written as:\n",
    "\n",
    "Vπ(s) = ∑ p(s',r | s, a) . (r + γVπ(s')) ,    for all s' ∈ S+ , r ∈ R \n",
    "\n",
    "In this case we are just multiplying the sum of the reward and the discounted value of next state by its corresponding probabilities and summing over all such possibilities to get the expected values.\n",
    "\n",
    "Similarly, for a *Stochastic policy*, it can be re-witten as:\n",
    "\n",
    "Vπ(s) = ∑ π(a|s) . p(s',r | s, a) . (r + γVπ(s')) ,    for all s' ∈ S+ , r ∈ R, a ∈ A(s) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
