{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** I have copied few lines directly from the book *Reinforcement Learning* by Richard S. Sutton and Andrew G. Barto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement learning is learning what to do—how to map situations to actions—so as to maximize a numerical reward signal. The learner is not told which actions to take, but instead must discover which actions yield the most reward by trying them. In the most interesting and challenging cases, actions may affect not only the immediate reward but also the next situation and, through that, all subsequent rewards. These two characteristics—trial-and-error search and delayed reward—are the two most important distinguishing features of reinforcement learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea is simply to capture the most important aspects of the real problem facing a learning agent interacting over time with its environment to achieve a goal. A learning agent must be able to sense the state of its environment to some extent and must be able to take actions that a.ect the state. The agent also must have a goal or goals relating to the state of the environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Why Reinforcement Learning?*\n",
    "\n",
    "In interactive problems it is often impractical to obtain examples of desired behavior (like what we do in Supervised Learning) that are both correct and representative of all the situations in which the agent has to act. In uncharted territory—where one would expect learning to be most beneficial—an agent must be able to learn from its own experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, Reinforcement learning, in layman terms involves an agent learning from its interaction with an environment, to maximize the rewards it earns over a period of time/steps.\n",
    "\n",
    "One of the challenges that arise in reinforcement learning, and not in other kinds of learning, is the trade-off between exploration and exploitation. To obtain a lot of reward, a reinforcement learning agent must prefer actions that it has tried in the past and found to be e.ective in producing reward. But to discover such actions, it has to try actions that it has not selected before. The agent has to exploit what it has already experienced in order to obtain reward, but it also has to explore in order to make better action selections in the future. The dilemma is that neither exploration nor exploitation can be pursued exclusively without failing at the task. The agent must try a variety of actions and progressively favor those that appear to be best. On a stochastic task, each action must be tried many times to gain a reliable estimate of its expected reward.\n",
    "\n",
    "- Exploration: Exploring potential hypothesis for how to choose actions\n",
    "- Exploitation: Exploiting limited knowledge about what is already known, as it should work well "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All reinforcement learning agents have explicit goals, can sense aspects of their environments, and can choose actions to influence their environments. Moreover, it is usually assumed from the beginning that the agent has to operate despite significant uncertainty about the environment it faces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elements of Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Major subelements of a Reinforcement Learning system:\n",
    "1. Agent\n",
    "2. Environment\n",
    "3. Policy\n",
    "4. Reward Signal\n",
    "5. Value Function\n",
    "6. Model (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Environment state can be interpreted as the information available to the agent about the environment it is in at a particular timestep.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Policy** defines the learning agent's behavior at a given time. It is dependent on the environment in the sense that it acts as a mapping of the actions the learning agent should take corresponding to that environment state. (Kinda like stimulus-response in humans)\n",
    "\n",
    "**Reward Signal** defines the goal of the learning agent in the current environment state. It is a single number for each time state. The agents objective is to maximize the cumulative reward it receives over the long run. Reward signals may be used to change the Policy.\n",
    "\n",
    "**Value Function** While Reward signal indicates what is good in the current state, the Value Function specifies what is good in the long run. Roughly speaking, the value of a state is the total amount of reward an agent can expect to accumulate in the future, starting from this state.\n",
    "\n",
    "Rewards are in a sense primary, whereas values, as predictions of rewards, are secondary. Without rewards there could be no values, and the only purpose of estimating values is to achieve more reward. Nevertheless, it is values with which we are most concerned when making and evaluating decisions. Action choices are made based on value judgments. We seek actions that bring about states of highest value, not highest reward, because these actions obtain the greatest amount of reward for us over the long run. Unfortunately, it is much harder to determine values than it is to determine rewards. Rewards are basically given directly by the environment, but values must be estimated and re-estimated from the sequences of observations an agent makes over its entire lifetime. In fact, the most important component of almost all reinforcement learning algorithms we consider is a method for efficiently estimating values.\n",
    "\n",
    "**Model** allows inferences to be made about how the environment will behave on taking an action. It might be able to predict next environment states and rewards, based on the current state and decided action. Models are used for planning, which means they help in deciding next course of action by considering future possibilities which have not been experienced before."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
