{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RL framework is characterized by an agent learning to interact with its environment. We treat time as discrete, and hence in terms of timesteps.\n",
    "\n",
    "Therefore, at Timestep 0, **T0**, the environment present its State,**S0** to the agent and the agent responds appropriately with an action, **A0**. This action changes the environment state and the environment gives its new state **S1**, at timestep **T1** to the agent, along with a reward **R1** corresponding to the action **A0** it took. This process is repeated over a number of timesteps, giving us a sequence of states, actions and rewards as **S0, A0, S1, A1, R1, S2, A2, R2, ....**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src= 'Images/Agent-Environment-Interaction.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reward is the most important characteristic for the agent as the goal of any agent is to maximize the cumulative reward it receives over all the timesteps. This can only be done by interacting with the environment as at every timestep the environment decides how much reward the agent receives. Therefore, the agent must play by the rules of the environment. But through a lot of interaction, the agent can learn those rules and choose appropriate actions to reach its goals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Episodic vs Continuous tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, what are tasks? A task is an instance of the Reinforcement learning problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When tasks have a well-defined ending point, they are called episodic tasks. For e.g., in a game of Counter Strike, an episode will end once all the players from a team are dead, whether it be the opposition or the player team, and at the end the environment gives us a score of how we did. Thus, an interaction ends at some timestep T, when the agent wins or loses, i.e reaches a _terminal state_, and the complete interaction from **S0, A0, S1, A1, R1, S2, A2, R2, ...., RT, ST** is known as an __*episode*__.\n",
    "\n",
    "When the episode ends, the agent looks at the cumulative reward it received to understand how well it did. And then with this experience gained, the agent can restart in the environment from scratch, but this time with the added knowledge of what happened in its previous life. In this way, over many lives, the agent gains experience and starts making better and better decisions. With this, the agent starts to improve its cumulative reward.\n",
    "\n",
    "A __*continuous*__ task is one where there is no well-defined ending point, and the agent keeps on learning over time to chose the best options, while simultaneously interacting with the environment. An example can be a stock-trading agent, which needs to buy and sell stocks in response to the behavior of the financial market."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An episodic task like playing chess, only gives a reward at the end of the episode (say win 1 , draw 0 , loss -1), and so the agent will have a difficult time in understanding such a problem, as the episode may involve a number of state-action-reward sequences, and the agent will have a difficult time in understanding which actions it took led to a win or loss. There can be a game where the agent made perfect moves throughout the episode but made one or two crucial mistakes leading to a loss, but it will only receive the reward -1 at the end of the episode, and hence will not understand where the mistake was made. Such problems, when the reward signal is largely uninformative in this way, we say that the task suffers the problem of **sparse rewards.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Reward Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reward hypothesis states that *\"All goals can be framed as the __maximization__ of expected cumulative rewards.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For e.g, the reward function that DeepMind came up with, to teach Locomotive behaviour(goal: walking) to a humanoid model as stated in their paper: __https://arxiv.org/pdf/1707.02286.pdf__  is quite simple and elegant. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Images/Deepmind-Reward.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cumulative Reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take the above locomotive task as example with the reward function as defined. If we wanted the maximum reward value at a particular timestep, we would try to walk as fast as possible, not caring about the other factors. But we want to maximize the cumulative reward after an episode, not the maximum reward in a given timestep. If we just start with the maximum speed, there is a chance that the humanoid destablizes and falls quickly, thus limiting the cumulative reward. Moreover, by focusing on just the immediate gain, the agent may not even learn to walk, which is our goal.\n",
    "\n",
    "Thus, it is clear that the agent can't just focus on individual timesteps, but needs to keep in mind all the timesteps. This holds true for all reinforcment learning agents in general. Actions have short and long term consequences and the agent needs to gain understanding of the complex effects its actions have on the environment. Having long term consequences in mind leads to stability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_How can an agent keep all timesteps in mind?_ \n",
    "\n",
    "At a particular timestep T, the agent only needs to think of the next timesteps, as the steps which were taken in previous timesteps are in the past and have been decided and only the future rewards are in the agents control. The sum of the rewards from the next timestep onwards as the **return** (denoted as G)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Images/Return.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And at an arbitrary timestep the agent will always choose an action towards maximizing the (expected) return. (Expected because generally the agent can't predict the future rewards with complete certainity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discounted Returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the agent at any arbitrary timestep is to always maximize the Cumulative Reward and hence the return G at that timestep. But it is not always exactly clear what the future holds, especially if the agent is still learning, proposing and testing its hypotheses and changing its stratergy (policy). The agent will usually have a better idea of the rewards it can gain in the near future timesteps rather than those which are far away. Thus, it makes sense to not give equal importance to all future rewards, but to give more value to rewards that come much sooner more highly. This is done by discounting the Rewards of the future timesteps by multiplying them successively with a discount rate to get the **Discounted return** G, as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Images/Discounted-Return.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_How to select the discount rate?_\n",
    "\n",
    "Well, if we want to focus more on the near-future rewards, keep the discount rate low (towards 0) and if we want to have a return focusing over a long-time reward gain, then keep the discount rate higher (towards 1).\n",
    "With γ =0, the return will focus on just the most immediate reward (next timestep)\n",
    "\n",
    "**Note:** The _**discount rate**_ is not learned by the agent, but is provided by us as we try to refine the goal set for the agent. Usually nearer to 1 than to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important:** Discounting is particularly important in continuous tasks, as the timesteps in such cases are limitless!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Decision Process (MDP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MDPs are used to frame reinforcement learning problems of learning from interaction to achieve a goal. The learner and decision maker is called the agent, and the thing it interacts with, comprising evrything outside the agent, is called the environment.\n",
    "\n",
    "Now as stated above, at each timestep __t__, the environment presents its state as an input to the agent to take an action. This state can be denoted as __St__, which is a subset of the state space __*S*__, the set of all nonterminal states. (**Note:** In episodic tasks, we use __*S+*__ to refer to the set of all states, including terminal states.)\n",
    "\n",
    "On the basis of the state received by it, the agent then chooses to take an action __*At*__ corresponding to it, from the action space __*A*__, which is the set of possible actions available to the agent. (__Note:__ In the event that there are some states where only a subset of the actions are available, we can also use __*A(s)*__ to refer to the set of actions available in state _**s ∈ S**_.) On the basis of the action we get another state __S(t+1)__ from the state space __*S*__ and a reward, having numerical value __*R*__, which is a subset of all possible real numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a finite MDP, the sets of states, actions, and rewards (S, A, and R) all have a finite number of elements. In this case, the random variables **Rt** and **St** have well defined discrete probability distributions dependent only on the preceding state and action. That is, for particular values of these random variables, s ∈ S and r ∈ R, there is a probability of those values occurring at time t, given particular values of the preceding state and action:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Images/OneStepDynamics.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above equation, __*p*__ provides the dynamics of the MDP environment, with it representing the probability of getting the next state s' and reward r, given that the previous state was s and the action taken by the agent was a. An example can be:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Images\\MDPExample.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example, \n",
    "\n",
    "p (high, 4 ∣ high, search) = P (S(t+1) = high, R(t+1) = 4 ∣ S(t) = high, A(t) = search) = 0.7\n",
    "\n",
    "and \n",
    "\n",
    "p (low, 4 ∣ high, search) = P (S(t+1) = low, R(t+1) = 4 ∣ S(t) = high, A(t) = search) = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a Markov decision process, the probabilities given by p completely characterize the environment’s dynamics. That is, the probability of each possible value for St and Rt depends only on the immediately preceding state and action, St−1 and At−1, and, given them, not at all on earlier states and actions. This is best viewed a restriction not on the decision process, but on the state. The state must include information about all aspects of the past agent–environment interaction that make a di↵erence for the future. If it does, then the state is said to have the __*Markov property*__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, a (finite) Markov Decision Process (MDP) is defined by:\n",
    "- a (finite) set of states S (or S+, in the case of an episodic task)\n",
    "- a (finite) set of actions A\n",
    "- a set of rewards R\n",
    "- the one-step dynamics of the environment\n",
    "- the discount rate γ∈[0,1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
