{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay! So we now know how to frame any reinforcement problem in terms of a Markov Decision Process (MPD) as: \n",
    "- A set of states _S_\n",
    "- A set of actions _A_\n",
    "- A set of rewards _R_\n",
    "- A one-dynamic step of the environment, p(s',r | s,a) = P(S(t+1) = s', R(t+1) = r | S(t) = s, A(t) = a)\n",
    "- A discount rate γ∈[0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what will be the solution to it? Well, to find the solution all we need is a series of actions that needs to be learned by the agent towards the pursuit of its goal, in context with its current state and the states that follow.\n",
    "\n",
    "Therefore, as long as the agent can learn the appropriate action to the environment states that it can observe, we have a solution to our problem. This leads to the idea of a policy defining these actions. A policy can be:\n",
    "- Deterministic\n",
    "- Stochastic\n",
    "\n",
    "**Deterministic Policy:** It is basically a mapping of states to the corresponding actions which should be taken and can be represented as __π:S→A__. Example: π(low) = recharge , π(high) = explore\n",
    "\n",
    "**Stochastic Policy:** A stochastic policy basically tells us the probability of an action(for all the possible actions in the action space) being taken by an agent while in a particular state and is represented as __π:S×A→[0,1]__. Example: π(recharge ∣ low) = 0.5, π(wait ∣ low) = 0.4, π(search ∣ low) = 0.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Any deterministic Policy can be represented as a Stochastic Policy (Prob. for that particular action-state combination will be 1).\n",
    "\n",
    "Now that we know how to define the policy, another question arises. How to get the best policy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State-Value Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the best policy, we first need to define something called a **state-value function**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each state in the state space, the state-value function yields the expected return if the agent starts in that state and then follows the specified policy for all the timesteps, till termiation of that episode. It is denoted as Vπ.\n",
    "\n",
    "**Note:** The state-value function corresponds to a particular policy and changes if there is a change in the policy.\n",
    "\n",
    "It can be written down in notation form as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Images\\State-Value-Function.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman Expectation Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above definition of State value function, the Bellman equation states that to get the value of any state, we just need the expected sum of the immediate rewards and the discounted value of the state that follows. (We calculate the expected sum because generally, we are not 100% sure of the expected reward and the state that follows.)\n",
    "\n",
    "Vπ(s) = Eπ [G(t) | S(t) = s]\n",
    "      \n",
    "      = Eπ [R(t+1) + γR(t+2) + .... | S(t) = s]\n",
    "      \n",
    "      = Eπ [R(t+1) + γVπ(S(t+1)) | S(t) = s]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'Images\\Bellman-Equation.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But how do we calculate these expected values? This is where our policy comes into play.\n",
    "\n",
    "For a *Deterministic policy*:\n",
    "\n",
    "Vπ(s) = Eπ [R(t+1) + γVπ(S(t+1)) | S(t) = s]\n",
    "\n",
    "can be re-written as:\n",
    "\n",
    "Vπ(s) = ∑ p(s',r | s, a) . (r + γVπ(s')) ,    for all s' ∈ S+ , r ∈ R \n",
    "\n",
    "In this case we are just multiplying the sum of the reward and the discounted value of next state by its corresponding probabilities and summing over all such possibilities to get the expected values.\n",
    "\n",
    "Similarly, for a *Stochastic policy*, it can be re-witten as:\n",
    "\n",
    "Vπ(s) = ∑ π(a|s) . p(s',r | s, a) . (r + γVπ(s')) ,    for all s' ∈ S+ , r ∈ R, a ∈ A(s) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that we have defined the state-value function, we can now compare two different policies.\n",
    "\n",
    "A policy π' is said to be better than another policy π, if and only if Vπ'(s) >= Vπ(s) for all s ∈ S.\n",
    "\n",
    "Now, it might often be the case that two policies cannot be compared in such a manner. But there will always be one policy that is better than or equal to all the other policies, and this policy is known as the **optimal policy, π***.\n",
    "\n",
    "                                                π* >= π for all π\n",
    "\n",
    "It is guarenteed to exist, but may not be unique, and is the solution to our MDP and the best strategy to reach its goal.\n",
    "The corresponding optimal state-value function is denoted as **v***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action-Value Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Denoted by q, the action-value function is a function of the environmental state and the agent's action.\n",
    "\n",
    "For each state s and action a, the action-value function yields the expected return, if the agent starts in the state s, chooses an action a, and then follows the policy specified to choose actions for all future timesteps. It can be formulated as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'Images\\Action-Value-Function.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a similar fashion to the optimal state-value function, the optimal action-value function is referred as **q***.\n",
    "\n",
    "**Note:** Vπ(s) = qπ(s, π(s))     \n",
    "**How? Think!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the optimal policy from the optimal action-value function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the way an agent learns an optimal policy follows the following path:\n",
    "       \n",
    "                           Interaction with the environment -> q* -> π*\n",
    "                           \n",
    "For now, let's ignore how the interaction leads to the optimal action-value function, and assume that we already know __q*__. Then, we can quickly obtain the optimal policy using this.\n",
    "\n",
    "What we need to do is follow the actions which yields the highest expected return in each state, to get the optimal policy, which can be formulized as:\n",
    "\n",
    "                    π∗(s) = argmax q∗(s,a) for all s ∈ S, with the argmax considering all a ∈ A(s)\n",
    "                    \n",
    "In cases of similar returns from a state for multiple actions, we can assign the similarly good actions any probability of being picked, just ensuring that the sum of their total probability is 1 and all the other, not so good actions have a probability of 0. Let's showcase this via an example. The table below contains the optimal action-values for different action-state pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'Images\\Optimal-Policy-Example.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the optimal policy π∗ for the corresponding MDP must satisfy:\n",
    "- π∗(s1) = a2 (or, equivalently, π*(a2| s1) = 1), and \n",
    "- π∗(s2) = a3 (or, equivalently, π*(a3| s2) = 1)\n",
    "\n",
    "As for state s3, there are two actions a1 and a2, which are equally good, and so the agent can choose either action a1 or a2 under the optimal policy, but it can never choose action a3. That is, the optimal policy π∗ must satisfy:\n",
    "- π*(a1| s3) = p,\n",
    "- π*(a2| s3) = q, and\n",
    "- π*(a3| s3) = 0\n",
    "\n",
    "where p+q = 1 and p,q>=0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
